# Day 5 / Apr 8 (Fri)

Continuing the journey into the land of Spark SQL.

## Exercises

Working on [Exercises for Apache Sparkâ„¢ and Scala Workshops](https://jaceklaskowski.github.io/spark-workshop/exercises/).

1. [Converting Arrays of Strings to String](https://jaceklaskowski.github.io/spark-workshop/exercises/spark-sql-exercise-Converting-Arrays-of-Strings-to-String.html)
1. [Using explode Standard Function](https://jaceklaskowski.github.io/spark-workshop/exercises/spark-sql-exercise-Using-explode-Standard-Function.html)
1. [Difference in Days Between Dates As Strings](https://jaceklaskowski.github.io/spark-workshop/exercises/spark-sql-exercise-Difference-in-Days-Between-Dates-As-Strings.html)
1. [How to add days (as values of a column) to date?](https://jaceklaskowski.github.io/spark-workshop/exercises/sql/How-to-add-days-as-values-of-a-column-to-date.html)

### Limiting collect_set Standard Function

[Limiting collect_set Standard Function](https://jaceklaskowski.github.io/spark-workshop/exercises/sql/limiting-collect_set-standard-function.html)

#### slice

```text
scala> all.withColumn("only_first_three", slice($"all", 1, 3)).show
+--------------------+----------------+
|                 all|only_first_three|
+--------------------+----------------+
|     [0, 1, 2, 3, 4]|       [0, 1, 2]|
|[0, 1, 2, 3, 4, 5...|       [0, 1, 2]|
+--------------------+----------------+
```

#### UDF

```scala
val sliceUDF = udf { (ns: Seq[Int]) => ns.take(3) }
```

```text
scala> all.withColumn("only_first_three", sliceUDF($"all")).show
+--------------------+----------------+
|                 all|only_first_three|
+--------------------+----------------+
|     [0, 1, 2, 3, 4]|       [0, 1, 2]|
|[0, 1, 2, 3, 4, 5...|       [0, 1, 2]|
+--------------------+----------------+
```

---

```scala
val sliceUDF = udf { (ns: Seq[Int], begin: Int, end: Int) => ns.slice(begin, end) }
```

```text
scala> all.withColumn("only_first_three", sliceUDF($"all", lit(1), lit(3))).show
+--------------------+----------------+
|                 all|only_first_three|
+--------------------+----------------+
|     [0, 1, 2, 3, 4]|          [1, 2]|
|[0, 1, 2, 3, 4, 5...|          [1, 2]|
+--------------------+----------------+
```

---

```scala
val sliceUDF = udf { (ns: Seq[Int], howMany: Int) => ns.take(howMany) }
```

```text
scala> all.withColumn("only_first_three", sliceUDF($"all", lit(3))).show
+--------------------+----------------+
|                 all|only_first_three|
+--------------------+----------------+
|     [0, 1, 2, 3, 4]|       [0, 1, 2]|
|[0, 1, 2, 3, 4, 5...|       [0, 1, 2]|
+--------------------+----------------+
```

#### filter Standard Function

```text
scala> all.withColumn("only_first_three", filter($"all", (x, idx) => idx < 3 )).show
+--------------------+----------------+
|                 all|only_first_three|
+--------------------+----------------+
|     [0, 1, 2, 3, 4]|       [0, 1, 2]|
|[0, 1, 2, 3, 4, 5...|       [0, 1, 2]|
+--------------------+----------------+
```

```scala
import org.apache.spark.sql.Column
val krzysiekFilter: (Column, Column) => Column = (x, idx) => idx < 3
```

```text
scala> all.withColumn("only_first_three", filter($"all", krzysiekFilter)).show
+--------------------+----------------+
|                 all|only_first_three|
+--------------------+----------------+
|     [0, 1, 2, 3, 4]|       [0, 1, 2]|
|[0, 1, 2, 3, 4, 5...|       [0, 1, 2]|
+--------------------+----------------+
```

## Theory

1. [Basic Aggregation](https://jaceklaskowski.github.io/spark-workshop/slides/spark-sql-basic-aggregation.html)

## Homework

### Reading

1. Read the scaladoc of the following types in Spark SQL:
    - [org.apache.spark.sql.RelationalGroupedDataset]({{spark.docs}}/api/scala/org/apache/spark/sql/RelationalGroupedDataset.html)
    - [org.apache.spark.sql.types.StructType]({{spark.docs}}/api/scala/org/apache/spark/sql/types/StructType.html)
1. Read [Datetime Patterns for Formatting and Parsing]({{spark.docs}}/sql-ref-datetime-pattern.html)

### Exercise

1. [Using upper Standard Function](https://jaceklaskowski.github.io/spark-workshop/exercises/spark-sql-exercise-Using-upper-Standard-Function.html)

## Schedule

1. 8:30 - 9:00 Introduction
1. 9:00 - 10:00 Exercises
1. 10:00 - 10:30 Discussion
1. 10:30 - 10:40 Break
1. 11:50 - 12:40pm Lunch break
1. 12:40pm - 2:30pm Exercises
